version: '3.2'
services:
  # https://gitlab.com/rychly-edu/docker/docker-spark-app/container_registry
  # http://localhost:4040/
  sparkapp:
    # hadoop 3.2.0 bug: missing lib/native/lihdfs.so, so cannot use pyarrow hdfs -> use hadoop 3.1.*
    image: registry.gitlab.com/rychly-edu/docker/docker-spark-app:spark2.4-hadoop3.1
    environment:
      - MASTER_URL=spark://sparkmaster:7077
      - SPARK_JARS=/app/lib
      - SPARK_PYFILES=/app/lib
      - SPARK_APP=/app/main.py
      - WEBUI_PORT=4040
    ports:
      - "4040:4040"
      - "5432:5432"
    volumes:
      - type: bind
        source: ./volumes/app
        target: /app
        read_only: true
  # https://gitlab.com/rychly-edu/docker/docker-spark/container_registry
  # http://localhost:4040/
  sparkmaster:
    image: registry.gitlab.com/rychly-edu/docker/docker-spark:2.4.4-hadoop3.1
    environment:
      - ROLE=master
      - MASTER_PORT=7077
      - WEBUI_PORT=4040
#    ports:
#      - "7077:7077"
#      - "4040:4040"
  sparkworker:
    # hadoop 3.2.0 bug: missing lib/native/lihdfs.so, so cannot use pyarrow hdfs -> use hadoop 3.1.*
    image: registry.gitlab.com/rychly-edu/docker/docker-spark:2.4.4-hadoop3.1
    # to restart lost workers (i.e., workers exited with after a task or on a failure when asked to kill an executor)
    restart: always
    environment:
      - ROLE=worker
      - MASTER_URL=spark://sparkmaster:7077
      - WEBUI_PORT=4040
      - INSTALL_PKGS=python-libfwsi python-libregf
#    ports:
#      - "4040:4041"
  # https://gitlab.com/rychly-edu/docker/docker-hdfs/container_registry
  # http://localhost:9870/
  namenode:
    image: registry.gitlab.com/rychly-edu/docker/docker-hdfs:3.1.3
    environment:
      - ROLE=namenode
      - DFS_DEFAULT=hdfs://namenode:8020
      - NAME_DIRS=/home/hadoop/name
      - DFS_NAMENODES=rpc://namenode:8020,http://namenode:9870,https://namenode:9871
      - ADD_USERS=spark,hbase
#    ports:
#      - "8020:8020"
      - "9870:9870"
#      - "9871:9871"
    volumes:
      - ./volumes/namenode:/home/hadoop/name
  datanode:
    image: registry.gitlab.com/rychly-edu/docker/docker-hdfs:3.1.3
    environment:
      - ROLE=datanode
      - DFS_DEFAULT=hdfs://namenode:8020
      - DATA_DIRS=/home/hadoop/data
      - DFS_DATANODES=data://0.0.0.0:9866,http://0.0.0.0:9864,https://0.0.0.0:9865,ipc://0.0.0.0:9867
#    ports:
#      - "9866"
#      - "9864"
#      - "9865"
#      - "9867"
    volumes:
      - ./volumes/datanode1:/home/hadoop/data
